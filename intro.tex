\section{Introduction}

The P4 language~\cite{p4:2014} is increasingly used to to define how packets are processed by a variety of programmable switches and NICs (network interface cards)~\cite{p4org, p4runtime, Stratum}. P4 is being used by networking researchers and the networking industry to define the forwarding pipeline of software switches~\cite{Pisces}, FPGAs~\cite{SDNet}, programmable ASICs~\cite{Tofino, Xpliant} and NICs~\cite{Netronome}. To date, though, the P4 language has been limited to defining how packets are parsed by a `match + action' pipeline. Much less attention has been focused on traffic management and how individual packets are scheduled. For example, current programmable ASICs only support scheduling algorithms chosen from among a fixed, popular set (e.g. strict priorities, WFQ, fixed-rate), and do not allow P4 programmers to define their own packet scheduling algorithm. %Typically, switches and routers have fixed scheduling policies that have some degree of control (e.g., priorities, weights), but not in terms of defining new scheduling algorithms.
This means adding new scheduling algorithms requires a long design cycle time, hindering the development of new ideas for specific use cases. It has proven hard to define an abstract forwarding model for traffic management that does not hamper efficient implementation at high line rates.  So traffic managers have traditionally been hard-coded, based on carefully hand-crafted designs.

The authors of~\cite{pifo2016} proposed using the PIFO queue as a general abstraction of packet scheduling. PIFOs provide both an abstract model (that can be programmed by extending the P4 language) as well as a potentially efficient hardware implementation. Originally introduced in~\cite{pifo1999}, PIFO is an abstract queuing discipline that includes, as special cases, most of the commonly used scheduling algorithms, such as strict priority queues, deficit round robin (DRR), minimum data rate, etc. In~\cite{pifo2016}, the authors extended the single PIFO queue to include hierarchical scheduling algorithms; for example, two strict priority queueing classes, where the flows in each class are scheduled using WFQ. 

In a basic PIFO queue, the departure order (or {\em rank}) of a packet is calculated when it arrives, and it is ``pushed-in'' to its  correct location in the queue. Packets are always dequeued from the head. In~\cite{pifo2016} the scheduling and shaping transactions are expressed using a Domino program ~\cite{domino:2015} that calculates the packet's rank and hence identifies where it should be inserted into the queue. The authors' PIFO queue was implemented as a series of registers that are compared in parallel; they can be shifted, starting from any position, to make room for a new packet to be inserted.  While a register-based implementation may work well for an ASIC, it does not work well for a field-programmable gate arrays (FPGA) which allows less flexible use of its resources, making it harder to fit and meet timing constraints. We therefore set out to create an implementation for FPGAs (the NetFPGA SUME~\cite{SUME} platform in particular), allowing others to reproduce, reuse and build upon our work. 

Our PIFO queue design uses {\em block random access memory} (BRAM), which is an abundant resource in FPGAs.  BRAMs require sequential memory accesses, making them less parallel than registers, but require much less chip routing resources.   When implemented using BRAM, the search, insert, and remove operations require multiple memory accesses. 

To mitigate the increased access times, our design is based on a deterministic skip list~\cite{det-skip-list}, to insert new packets quickly and within a bounded time. Our design uses multiple skip lists in parallel to (always) meet the 10Gb/s line-rate. It also uses a small register-based cache to make sure packets are available at the head of the queue which ensures the PIFO queue is work-conserving at all times. We parameterized each major block to achieve different performance targets while scaling resources conveniently.

We first wrote a behavioral and modular Python model as a proof of concept.  We then used the SimPy package ~\cite{SimPy}, a discrete event simulation library for Python, to model the PIFO operations (search, insertion and removal) into the block RAMs. A key design parameter is the number of memory accesses required to meet line-rate. We varied the design parameters to study their effect on performance using Python SimPy and chose an optimal set of parameters taking into account FPGA resource utilization.  

In the final design step we translated the Python SimPy code into Verilog, which was straightforward and automatic since the processing logic and memory accesses were already designed, implemented, and verified.  We integrated the Verilog implementation with an existing P4 Switch and targeted the NetFPGA SUME development board~\cite{SUME}. 

FPGAs are increasingly used to build switches. In the past, there were typically 10-20x slower than dedicated ASICs; but today, because of hardened FPGA blocks for serial I/O and lookup tables, the gap has closed to about 2-3x. We therefore believe FPGAs (and hence NetFPGA) are a realistic way for researchers and industry to build switches and NICs. 

The main contributions of this paper are:
\begin{enumerate}
\item A practical implementation of a programmable scheduler using a PIFO on an FPGA.
\item The first use of deterministic skip lists to provide deterministic line-rate performance for processing packets.
\item The first PIFO design where no assumptions are made about ranks and flows. That is, each element in the PIFO can belong to a separate flow, which provides much more fine grained control over scheduling order than previous proposals.
\item First demonstration of using a PIFO to implement specific scheduling algorithms, including strict priority, weighted round robin and shortest remaining processing time (SRPT).
\item We contribute a complete system to the community with verified end-to-end functionality, as well as a software environment to reproduce and build upon the key results presented in this paper.
\end{enumerate}

In the following sections we describe scheduling in general in Section~2 and the PIFO queue in Section~3.  In Section~4, we describe our implementations and the justification for our design decisions.  Section~5 contains the evaluation results from the Verilog simulations and Section~6, the measurements results from our hardware prototype on the NetFPGA-SUME card.   In sections 7 and 8 we discuss extensions of our work and position our design relative to other related work.
